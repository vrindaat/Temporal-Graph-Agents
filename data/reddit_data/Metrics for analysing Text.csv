S. No.,method_family,metric_name,purpose,formula/definition,tool/library,input_granularity(token/sentence/doc),strengths,limitations,domain_notes,license,citation
1,lexical,BLEU,BLEU is useful to check for quality and accuracy compared to a human-created reference.,Compares the n-grams (word sequences) in the generated text to the n-grams in the reference text(s).,nltk,sentence,Measures how well the generated text resembles a correct translation or a specific example.,"BLEU may not accurately score a transferred sentence if
it does not have a high n-gram overlap with the reference or the source.",NA,open-source,"Chen, J. (2024). Lmstyle benchmark: Evaluating text style transfer for chatbots. arXiv preprint arXiv:2403.08943."
2,semantic,BERTscore,"evaluates the quality of text summarization, measuring how similar the text summary is to the original text.","extract the embedding of the transferred sentence, the source,
and the reference to calculate their cosine distance",transformers & bert_score,sentence,"BERTScore uses the power of BERT, a state-of-the-art transformer-based model developed by Google, to understand the semantic meaning of words in a sentence. This leads to a more accurate representation of text similarity compared to traditional methods that rely on syntactic structures.","1. It can be biased towards models that are more similar to its own underlying model.
2. BERTScore doesn’t take into account the syntactic structure of the sentence. Which can lead to incorrect evaluations in cases where the syntactic structure of the sentences is different but they convey the same meaning.
3. It might perform poorly on tasks that require understanding the context beyond the individual words, such as idiomatic expressions or cultural references",NA,open-source,
3,llmness,Perplexity,"Perplexity (PPL) is a metric for evaluating the fluency and naturalness of text, and it can be applied to compare different writing styles.","Perplexity is mathematically defined as the exponentiation of the average negative log-likelihood (which is the cross-entropy) of a sequence of words. 
The perplexity (PPL) score is a metric used to evaluate how well a probability model, particularly a language model, predicts a sample of text",transformers  ,sentence,Perplexity is calculated quickly and can be used during model training to instantly assess how well the model is performing.,"1. Perplexity measures the ability to predict sequences but doesn’t align with specific task objectives
2. Larger vocabularies can inflate Perplexity, even for reasonably good models, making direct comparisons tricky.
3. Low Perplexity ensures fluency but doesn’t account for whether outputs are relevant or meaningful for the given context.",NA,open-source,
4,stylistic,Yule's K,"Yule's K is a statistical measure used to determine the lexical richness or vocabulary diversity of a text, with a higher value indicating a less diverse vocabulary.","It assesses the distribution of word occurrences, calculating how often words are repeated. A text with many repeated words has a lower K characteristic (less rich vocabulary), while a text with less repetition has a higher K characteristic.",NO TOOL AVAILABLE,doc,,,,,"Yule, G. U. (1939). On Sentence-Length as a Statistical Characteristic of Style in Prose: With Application to Two Cases of Disputed Authorship. Biometrika, 30(3/4), 363–390. https://doi.org/10.2307/2332655"
5,stylistic,Burrow's Delta,Burrows' Delta is a stylometric method for authorship attribution,"It works by standardizing word frequencies, creating a numerical representation of each text, and then calculating the distance between these representations to find the closest stylistic match, which helps identify an anonymous author. ",fastylometry,doc,,,used for authorship attribution,open-source,"John Burrows, ‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship, Literary and Linguistic Computing, Volume 17, Issue 3, September 2002, Pages 267–287, https://doi.org/10.1093/llc/17.3.267"
6,stylistic,LWBOW(Locally weighted Bag of Words,to create a more robust document representation than standard Bag of Words (BoW),"The model applies local smoothing to the word counts to create a smoother, more continuous representation than the discrete counts of a standard bag-of-words model.",scikit-learn,doc,preserves more information by accounting for some sequential context,preserves more information by accounting for some sequential context,nlp tasks,open-source,"Escalante, H. J., Solorio, T., & Montes, M. (2011, June). Local histograms of character n-grams for authorship attribution. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies (pp. 288-298)."
7,readablity,Flesch Reading Ease Score,"measures how easy or difficult a piece of text is to understand, with a score ranging from 0–100","The formula is: 206.835 – (1.015 × ASL) – (84.6 × ASW), where ASL is the average sentence length (total words divided by total sentences) and ASW is the average number of syllables per word (total syllables divided by total words)",textstat,doc,simplicity and wide applicability for improving clarity by focusing on sentence and word length,"Ignores other factors like the actual meaning of the words and the complexity of the sentence structure, lacks reader context",NA,open-source,"Flesch, R. (1948). A new readability yardstick. Journal of applied psychology, 32(3), 221."
8,readablity,Flesch Kincaid grade level score,determines the U.S. school grade level required to understand a text,The formula is : 0.39 x (average sentence length)+11.8 x (average syllables per word)-15.59.,textstat,doc,its simplicity in helping ensure content is understandable for a target audience by focusing on sentence and syllable length,"overlooks other crucial factors like prior knowledge, content complexity, and the overall coherence of the writing. ",NA,open-source,
9,readablity,Gunning Fog Index,"used to measure the readability of a text, estimating the years of formal education needed to understand it",The Formula: 0.4 x (Average Sentence Length+Percentage of Hard Words),textstat,doc,"provides a quick, objective measure for ensuring text is accessible to a target audience, especially for technical documents or public information that needs to be clear and easily understood.","can be misleading by oversimplifying complex issues, as it doesn't account for the reader's background knowledge, sentence structure, or the actual difficulty of the subject matter. ",NA,open-source,
10,readablity,SMOG index,provides a numerical score that approximates the U.S. school grade level needed to understand the text.,,textsat,doc,simplicity and high correlation with reading comprehension,"does not account for other factors that influence readability, such as vocabulary diversity, sentence structure, context, tone, or overall writing quality, and assumes longer words are more complex.",NA,open-source,
11,readablity,Dale Chall Readability,To provide a U.S. grade-level score for a text to gauge its readability,Formula: (Score=0.1579 x (% of Difficult Words)+0.0496 x (Average Sentence Length)+3.6365),textstat,doc,focuses on familiar words and uses a predefined list to identify difficult terms,"does not account for factors beyond sentence length and word familiarity, such as tone, subject matter, and vocabulary diversity",NA,open-source,"Dale, E., & Chall, J. S. (1949). The concept of readability. Elementary English, 26(1), 19-26."
12,clickbait,Clickbait dataset,Train a supervised model to detect whether text is clickbait or not,NA,Kaggle,sentence,Equal distribution of clickbait and non-clickbait news headlines from quality sources,Could be prone to bias,NA,open-source,https://www.kaggle.com/datasets/amananandrai/clickbait-dataset/data
13,polarization,eMFD Score,Used to measure the morality of text,"Calculates 5 probabilities of each word in the sentence/doc based on the Moral Foundations Theory. Uses various methods to assign one score(average/max). Finally, a single class is assigned(moral/immoral)",emfdscore,doc,,"may not generalize to other types of text well, since it is based on word-count, it may not work well with short texts.",News data,open-source,"Hopp, F. R., Fisher, J. T., Cornell, D., Huskey, R., & Weber, R. (2020). The extended Moral Foundations Dictionary (eMFD): Development and applications of a crowd-sourced approach to extracting moral intuitions from text. Behavior Research Methods, https://doi.org/10.3758/s13428-020-01433-0"
14,polarization,MFRC,"Reddit dataset with human annotated data, can be used to calculate morality",Use an embedding model like BERT to convert text into embeddings train a classifier model on this dataset and use it to classify the embeddings,sentence-transformer,doc,,,Political data,open-source,"Trager, J., Ziabari, A. S., Davani, A. M., Golazizian, P., Karimi-Malekabadi, F., Omrani, A., ... & Dehghani, M. (2022). The moral foundations reddit corpus. arXiv preprint arXiv:2208.05545."
15,semantic,SBERT Cosine Similarity Score,Used to find the semantic similarity between two pieces of text,Uses a Siamese network model to convert the text into embeddings in a vector space. Cosine similarity is used to find the angle between the two vector and a score is given based on how far or close the vectors are to each other,sentence-transformer,doc,is robust to differences in sentence length," when sentences have similar vocabulary but different meanings or when negation is complex, it can incorrectly identify them as similar. ",NA,open-source,"Reimers, N., & Gurevych, I. S. B. (1908). Sentence embeddings using siamese BERT-networks. arXiv 2019. arXiv preprint arXiv:1908.10084, 10."
16,lexical,Type-Token Ratio (TTR),Measure vocabulary richness,,"spacy, nltk",doc,Simple proxy for lexical diversity,Sensitive to text length (longer texts = lower TTR), ,,Templin(1957)
17,stylistic,Hedge/Booster Density,Measure uncertainty vs. confidence,,spacy + Lexicons,doc,Captures writer's confidence level,Relies on fixed lists; context-dependent,,,Hyland (2005)
18,sentiment,VADER Compound,Detect polarity (-1 to +1),,vaderSentiment,sentence,Tuned for social media & news; handles negation,Misses sarcasm; rule-based limits,,,Hutto & Gilbert (2014)
19,toxicity,Toxicity Probability,"Detect insults, threats, profanity",,unitaryai/detoxify,sentence,State-of-the-art accuracy on abusive language,Can be biased against certain identity terms,,,Hanu & UnitaryAI (2020)
20,topic & content,Entity Jaccard Index,Measure if key subjects (people/orgs) changed,,spacy NER,doc," Good proxy for ""did the subject matter change?""",Misses abstract topic shifts,,,Jaccard (1912)
21,structure,Connective Density,"Measure text cohesion (therefore, however)",,spacy,doc,Indicates logical complexity,Simple counts miss misused connectives,,,Halliday & Hasan (1976)
22,,,,,,,,,,,
23,,,,,,,,,,,
24,,,,,,,,,,,
25,,,,,,,,,,,
26,,,,,,,,,,,
27,,,,,,,,,,,
28,,,,,,,,,,,
29,,,,,,,,,,,
30,,,,,,,,,,,
31,,,,,,,,,,,
32,,,,,,,,,,,