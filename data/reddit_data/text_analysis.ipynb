{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd472290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.26.4)\n",
      "Requirement already satisfied: spacy in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.8.11)\n",
      "Requirement already satisfied: textstat in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.7.11)\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.3.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.7.0)\n",
      "Requirement already satisfied: detoxify in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.5.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (67.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: pyphen in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from textstat) (3.9.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.94 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from detoxify) (0.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: wrapt in c:\\users\\vrinda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of dipy: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    fury (>=0.8.0scikit-learn); extra == 'all'\n",
      "         ~~~~~~~~^\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Vrinda\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 3.1/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 17.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of dipy: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    fury (>=0.8.0scikit-learn); extra == 'all'\n",
      "         ~~~~~~~~^\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Vrinda\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy spacy textstat vaderSentiment sentence-transformers scikit-learn detoxify transformers torch\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db36da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import textstat\n",
    "import torch\n",
    "import re\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from detoxify import Detoxify\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, pipeline\n",
    "import math\n",
    "import csv\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c24656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "852af30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveComparator:\n",
    "    def __init__(self, emfd_path='Datasets/emfd_scoring.csv', clickbait_path='Datasets/clickbait_data.csv'):\n",
    "        print(\"Initializing Models... (This may take a moment)\")\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except:\n",
    "            print(\"   Downloading spacy model...\")\n",
    "            from spacy.cli import download\n",
    "            download(\"en_core_web_sm\")\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            \n",
    "        self.sentiment = SentimentIntensityAnalyzer()\n",
    "        self.sbert = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        \n",
    "        try:\n",
    "            self.nli_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=-1)\n",
    "        except:\n",
    "            self.nli_classifier = None\n",
    "            print(\"   Warning: NLI model failed to load.\")\n",
    "\n",
    "        self.tox = None\n",
    "        try:\n",
    "            self.tox = Detoxify('original')\n",
    "        except:\n",
    "            print(\"   Warning: Detoxify failed to load.\")\n",
    "\n",
    "        try:\n",
    "            self.gpt2_id = \"gpt2\"\n",
    "            self.gpt2_model = GPT2LMHeadModel.from_pretrained(self.gpt2_id)\n",
    "            self.gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(self.gpt2_id)\n",
    "        except:\n",
    "            self.gpt2_model = None\n",
    "\n",
    "        self._load_emfd(emfd_path)\n",
    "        self._train_clickbait(clickbait_path)\n",
    "\n",
    "    def _load_emfd(self, path):\n",
    "        self.emfd_dict = {}\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                self.emfd_dict = df.set_index('word').to_dict('index')\n",
    "            except: pass\n",
    "\n",
    "    def _train_clickbait(self, path):\n",
    "        self.cb_model = None\n",
    "        if path and os.path.exists(path):\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                self.cb_model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "                self.cb_model.fit(df['text'], df['label'])\n",
    "            except: pass\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        text = re.sub(r'<[^>]+>', '', text) \n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        doc = self.nlp(text)\n",
    "        sents = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 0]\n",
    "        tokens = [t.text.lower() for t in doc if not t.is_punct and not t.is_space]\n",
    "        return doc, sents, tokens\n",
    "\n",
    "    # ==========================================\n",
    "    # HELPER: Sentence Alignment\n",
    "    # ==========================================\n",
    "    def align_sentences(self, sents_o, sents_e):\n",
    "        \"\"\"\n",
    "        Aligns sentences from Doc O to Doc E using SBERT cosine similarity.\n",
    "        Returns a list of tuples: [(sent_o, sent_e, similarity_score), ...]\n",
    "        \"\"\"\n",
    "        if not sents_o or not sents_e:\n",
    "            return []\n",
    "        \n",
    "        # Encode both sets\n",
    "        emb_o = self.sbert.encode(sents_o, convert_to_tensor=True)\n",
    "        emb_e = self.sbert.encode(sents_e, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cosine_scores = util.cos_sim(emb_o, emb_e)\n",
    "        \n",
    "        # For each sentence in O, find best match in E\n",
    "        aligned = []\n",
    "        for i, sent_o in enumerate(sents_o):\n",
    "            best_idx = torch.argmax(cosine_scores[i]).item()\n",
    "            score = cosine_scores[i][best_idx].item()\n",
    "            aligned.append((sent_o, sents_e[best_idx], score))\n",
    "            \n",
    "        return aligned\n",
    "\n",
    "    # ==========================================\n",
    "    # CORE METRICS (Single Representative)\n",
    "    # ==========================================\n",
    "    \n",
    "    # A) Lexical: TTR\n",
    "    def get_lexical_metrics(self, doc, sents):\n",
    "        res = []\n",
    "        for sent in doc.sents:\n",
    "            tokens = [t.text.lower() for t in sent if not t.is_punct]\n",
    "            ttr = len(set(tokens)) / len(tokens) if tokens else 0.0\n",
    "            res.append(ttr)\n",
    "        return res\n",
    "\n",
    "    # B) Readability: FKGL\n",
    "    def get_readability_metrics(self, sentences):\n",
    "        res = []\n",
    "        for s in sentences:\n",
    "            try:\n",
    "                val = textstat.flesch_kincaid_grade(s)\n",
    "                res.append(val)\n",
    "            except:\n",
    "                res.append(0.0)\n",
    "        return res\n",
    "\n",
    "    # C) Stylometry: Burrows Delta (Distance)\n",
    "    def get_burrows_delta(self, tokens_o, tokens_e):\n",
    "        all_t = tokens_o + tokens_e\n",
    "        if not all_t: return 0.0\n",
    "        common = [w for w, c in Counter(all_t).most_common(50)]\n",
    "        \n",
    "        n_o, n_e = len(tokens_o), len(tokens_e)\n",
    "        if n_o == 0 or n_e == 0: return 0.0\n",
    "        \n",
    "        f_o = np.array([tokens_o.count(w)/n_o for w in common])\n",
    "        f_e = np.array([tokens_e.count(w)/n_e for w in common])\n",
    "        \n",
    "        mean_freq = (f_o + f_e) / 2\n",
    "        std_freq = np.std([f_o, f_e], axis=0) + 1e-9\n",
    "        \n",
    "        z_o = (f_o - mean_freq) / std_freq\n",
    "        z_e = (f_e - mean_freq) / std_freq\n",
    "        \n",
    "        return np.mean(np.abs(z_o - z_e))\n",
    "\n",
    "    # D) Sentiment: Compound\n",
    "    def get_sentiment_emotion(self, sentences):\n",
    "        res = []\n",
    "        for s in sentences:\n",
    "            res.append(self.sentiment.polarity_scores(s)['compound'])\n",
    "        return res\n",
    "\n",
    "    # E) Sensationalism: Clickbait\n",
    "    def get_sensationalism(self, sentences):\n",
    "        res = []\n",
    "        for s in sentences:\n",
    "            val = 0.0\n",
    "            if self.cb_model:\n",
    "                try: val = self.cb_model.predict_proba([s])[0][1]\n",
    "                except: pass\n",
    "            res.append(val)\n",
    "        return res\n",
    "\n",
    "    # F) Framing: Moral Score\n",
    "    def get_framing(self, doc):\n",
    "        res = []\n",
    "        for sent in doc.sents:\n",
    "            tokens = [t.text.lower() for t in sent if not t.is_punct]\n",
    "            score = 0.0\n",
    "            for t in tokens:\n",
    "                if t in self.emfd_dict:\n",
    "                    vals = self.emfd_dict[t]\n",
    "                    score += sum([v for k,v in vals.items() if k.endswith('_p')])\n",
    "            res.append(score / len(tokens) if tokens else 0.0)\n",
    "        return res\n",
    "\n",
    "    # G) Toxicity\n",
    "    def get_toxicity(self, sentences):\n",
    "        res = []\n",
    "        for s in sentences:\n",
    "            val = 0.0\n",
    "            if self.tox:\n",
    "                try: val = self.tox.predict(s[:512])['toxicity']\n",
    "                except: pass\n",
    "            res.append(val)\n",
    "        return res\n",
    "\n",
    "    # H) Topic Shift (Entities) - Helper\n",
    "    def get_entities(self, doc):\n",
    "        return [e.text.lower() for e in doc.ents]\n",
    "\n",
    "    # J) Discourse: Density\n",
    "    def get_discourse(self, doc):\n",
    "        markers = {'however', 'therefore', 'thus', 'moreover', 'because', 'since', 'but', 'and', 'so'}\n",
    "        res = []\n",
    "        for sent in doc.sents:\n",
    "            cnt = len([t for t in sent if t.text.lower() in markers])\n",
    "            res.append(cnt / len(sent) if len(sent) > 0 else 0.0)\n",
    "        return res\n",
    "\n",
    "    # K) Factuality - Helper\n",
    "    def get_factuality(self, text):\n",
    "        nums = re.findall(r'\\d+(?:[.,]\\d+)?', text)\n",
    "        return set(nums)\n",
    "\n",
    "    # L) LLM-ness: Perplexity\n",
    "    def get_perplexity(self, sentences):\n",
    "        res = []\n",
    "        if not self.gpt2_model: return [0.0]*len(sentences)\n",
    "        for s in sentences:\n",
    "            if not s.strip(): \n",
    "                res.append(0.0)\n",
    "                continue\n",
    "            try:\n",
    "                enc = self.gpt2_tokenizer(s, return_tensors='pt')\n",
    "                with torch.no_grad():\n",
    "                    out = self.gpt2_model(enc.input_ids, labels=enc.input_ids)\n",
    "                    res.append(math.exp(out.loss.item()))\n",
    "            except:\n",
    "                res.append(0.0)\n",
    "        return res\n",
    "\n",
    "    # ==========================================\n",
    "    # AGGREGATION & BOOTSTRAPPING\n",
    "    # ==========================================\n",
    "    def _bootstrap_ci(self, scores_o, scores_e, n_boot=1000):\n",
    "        so = np.array(scores_o, dtype=float)\n",
    "        se = np.array(scores_e, dtype=float)\n",
    "        \n",
    "        if len(so)==0: so = np.array([0.0])\n",
    "        if len(se)==0: se = np.array([0.0])\n",
    "\n",
    "        m_o = np.mean(so)\n",
    "        m_e = np.mean(se)\n",
    "        diff = m_o - m_e\n",
    "        \n",
    "        # Cohen's d\n",
    "        n1, n2 = len(so), len(se)\n",
    "        pooled_std = np.sqrt((np.var(so, ddof=1) + np.var(se, ddof=1))/2) + 1e-9\n",
    "        cohens_d = diff / pooled_std\n",
    "\n",
    "        # Access random index instead of full shuffle for speed\n",
    "        try:\n",
    "            idx_o = np.random.randint(0, n1, (n_boot, n1))\n",
    "            idx_e = np.random.randint(0, n2, (n_boot, n2))\n",
    "            \n",
    "            means_o_boot = np.mean(so[idx_o], axis=1)\n",
    "            means_e_boot = np.mean(se[idx_e], axis=1)\n",
    "            boot_diffs = means_o_boot - means_e_boot\n",
    "            \n",
    "            ci_low = np.percentile(boot_diffs, 2.5)\n",
    "            ci_high = np.percentile(boot_diffs, 97.5)\n",
    "        except:\n",
    "            ci_low, ci_high = 0.0, 0.0\n",
    "        \n",
    "        try:\n",
    "            _, p_val = stats.ttest_ind(so, se, equal_var=False)\n",
    "        except: p_val = 1.0\n",
    "\n",
    "        return {\n",
    "            \"mean_o\": m_o, \"mean_e\": m_e, \"diff\": diff, \n",
    "            \"ci_low\": ci_low, \"ci_high\": ci_high, \"p\": p_val, \"es\": cohens_d\n",
    "        }\n",
    "\n",
    "    # ==========================================\n",
    "    # RUNNER\n",
    "    # ==========================================\n",
    "    def run_pair(self, text_o, text_e, pair_id):\n",
    "        doc_o, sents_o, toks_o = self._preprocess(text_o)\n",
    "        doc_e, sents_e, toks_e = self._preprocess(text_e)\n",
    "        \n",
    "        # Mapping metric name to (ScoreO, ScoreE)\n",
    "        distro_metrics = {\n",
    "             \"Lexical (TTR)\": (self.get_lexical_metrics(doc_o, sents_o), self.get_lexical_metrics(doc_e, sents_e)),\n",
    "             \"Readability (FKGL)\": (self.get_readability_metrics(sents_o), self.get_readability_metrics(sents_e)),\n",
    "             \"Sentiment (Compound)\": (self.get_sentiment_emotion(sents_o), self.get_sentiment_emotion(sents_e)),\n",
    "             \"Clickbait Score\": (self.get_sensationalism(sents_o), self.get_sensationalism(sents_e)),\n",
    "             \"Moral Framing\": (self.get_framing(doc_o), self.get_framing(doc_e)),\n",
    "             \"Toxicity\": (self.get_toxicity(sents_o), self.get_toxicity(sents_e)),\n",
    "             \"Discourse Density\": (self.get_discourse(doc_o), self.get_discourse(doc_e)),\n",
    "             \"Perplexity\": (self.get_perplexity(sents_o), self.get_perplexity(sents_e))\n",
    "        }\n",
    "        \n",
    "        all_stats = {}\n",
    "        for name, (dist_o, dist_e) in distro_metrics.items():\n",
    "            all_stats[name] = self._bootstrap_ci(dist_o, dist_e)\n",
    "\n",
    "        # -- Single Value Metrics --\n",
    "        # C) Stylometry: Burrows Delta\n",
    "        burrows = self.get_burrows_delta(toks_o, toks_e)\n",
    "\n",
    "        # H) Topic Shift (Entity Jaccard)\n",
    "        ents_o = set(self.get_entities(doc_o))\n",
    "        ents_e = set(self.get_entities(doc_e))\n",
    "        u = len(ents_o.union(ents_e))\n",
    "        jaccard = len(ents_o.intersection(ents_e))/u if u else 0\n",
    "        \n",
    "        # I) Semantic Sim\n",
    "        if len(sents_o) > 0 and len(sents_e) > 0:\n",
    "            emb_o = self.sbert.encode(sents_o)\n",
    "            emb_e = self.sbert.encode(sents_e)\n",
    "            sim_matrix = util.cos_sim(emb_o, emb_e).numpy()\n",
    "            sem_sim = float(np.mean(np.max(sim_matrix, axis=1)))\n",
    "        else:\n",
    "            sem_sim = 0.0\n",
    "        \n",
    "        # K) Factuality (Number Jaccard)\n",
    "        nums_o = self.get_factuality(text_o)\n",
    "        nums_e = self.get_factuality(text_e)\n",
    "        nu = len(nums_o.union(nums_e))\n",
    "        num_jac = len(nums_o.intersection(nums_e))/nu if nu else 0\n",
    "\n",
    "        single_stats = {\n",
    "            \"Style (Burrows Delta)\": burrows,\n",
    "            \"Entity Jaccard\": jaccard,\n",
    "            \"Semantic Similarity\": sem_sim,\n",
    "            \"Numeric Jaccard\": num_jac\n",
    "        }\n",
    "\n",
    "        # Viz Data for Radar\n",
    "        viz_keys = [\"Readability (FKGL)\", \"Sentiment (Compound)\", \"Toxicity\", \"Clickbait Score\", \"Moral Framing\"]\n",
    "        viz_data = {}\n",
    "        for k in viz_keys:\n",
    "             viz_data[k] = {'mean_o': all_stats[k]['mean_o'], 'mean_e': all_stats[k]['mean_e']}\n",
    "\n",
    "        # Generate new visualizations\n",
    "        self.plot_entity_heatmap(doc_o, doc_e, pair_id)\n",
    "        self.plot_similarity_matrix(sents_o, sents_e, pair_id)\n",
    "        self.plot_alignment_table(sents_o, sents_e, pair_id)\n",
    "\n",
    "        return all_stats, single_stats, viz_data\n",
    "\n",
    "    def visualize(self, viz_data, pair_id):\n",
    "        # Radar Chart\n",
    "        # Map nice labels to key names in viz_data\n",
    "        key_map = [\n",
    "            (\"Readability\", \"Readability (FKGL)\"),\n",
    "            (\"Sentiment\", \"Sentiment (Compound)\"),\n",
    "            (\"Toxicity\", \"Toxicity\"),\n",
    "            (\"Clickbait\", \"Clickbait Score\"),\n",
    "            (\"Moral\", \"Moral Framing\")\n",
    "        ]\n",
    "        \n",
    "        labels = [x[0] for x in key_map]\n",
    "        keys = [x[1] for x in key_map]\n",
    "        \n",
    "        means_o = [viz_data.get(k, {}).get('mean_o', 0) for k in keys]\n",
    "        means_e = [viz_data.get(k, {}).get('mean_e', 0) for k in keys]\n",
    "        \n",
    "        # Normalize (Heuristic MinMax)\n",
    "        norm_o = [\n",
    "            min(max(means_o[0]/20, 0), 1), # fkgl 0-20\n",
    "            (means_o[1]+1)/2, # compound -1 to 1 -> 0 to 1\n",
    "            min(means_o[2], 1), # tox 0-1\n",
    "            min(means_o[3], 1), # clickbait 0-1\n",
    "            min(means_o[4]*5, 1) # moral often small, boost\n",
    "        ]\n",
    "        norm_e = [\n",
    "            min(max(means_e[0]/20, 0), 1),\n",
    "            (means_e[1]+1)/2,\n",
    "            min(means_e[2], 1),\n",
    "            min(means_e[3], 1),\n",
    "            min(means_e[4]*5, 1)\n",
    "        ]\n",
    "        \n",
    "        angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
    "        norm_o += norm_o[:1]; norm_e += norm_e[:1]; angles += angles[:1]\n",
    "        \n",
    "        plt.figure(figsize=(6,6))\n",
    "        ax = plt.subplot(111, polar=True)\n",
    "        ax.plot(angles, norm_o, label='Original')\n",
    "        ax.fill(angles, norm_o, alpha=0.25)\n",
    "        ax.plot(angles, norm_e, label='Rewrite')\n",
    "        ax.fill(angles, norm_e, alpha=0.25)\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(labels)\n",
    "        plt.title(f\"Radar Comparison: {pair_id}\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{pair_id}_radar.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def get_gltr_hist(self, text_o, text_e, pair_id):\n",
    "        if not self.gpt2_model: return\n",
    "        \n",
    "        def rank_text(txt):\n",
    "            # Limit to 1000 chars for speed\n",
    "            enc = self.gpt2_tokenizer(txt[:1000], return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                logits = self.gpt2_model(enc.input_ids).logits\n",
    "            ranks = []\n",
    "            for i in range(len(enc.input_ids[0])-1):\n",
    "                tid = enc.input_ids[0][i+1]\n",
    "                args = torch.argsort(logits[0,i], descending=True)\n",
    "                try: r = (args==tid).nonzero().item()\n",
    "                except: r=10000\n",
    "                ranks.append(r)\n",
    "            return ranks\n",
    "\n",
    "        ro = rank_text(text_o)\n",
    "        re_ = rank_text(text_e)\n",
    "        \n",
    "        def bucket(r): return 1 if r<10 else 2 if r<100 else 3 if r<1000 else 4\n",
    "        bo = [bucket(r) for r in ro]\n",
    "        be = [bucket(r) for r in re_]\n",
    "        \n",
    "        co = [bo.count(i) for i in [1,2,3,4]]\n",
    "        ce = [be.count(i) for i in [1,2,3,4]]\n",
    "        \n",
    "        x = np.arange(4)\n",
    "        plt.bar(x-0.2, co, 0.4, label='Original')\n",
    "        plt.bar(x+0.2, ce, 0.4, label='Rewrite')\n",
    "        plt.xticks(x, ['Top 10', 'Top 100', 'Top 1k', '>1k'])\n",
    "        plt.title(f\"GLTR Distribution: {pair_id}\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{pair_id}_gltr_hist.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_entity_heatmap(self, doc_o, doc_e, pair_id):\n",
    "        ents_o = [e.text.lower() for e in doc_o.ents]\n",
    "        ents_e = [e.text.lower() for e in doc_e.ents]\n",
    "        \n",
    "        # Get top 20 most common entities combined\n",
    "        all_ents = ents_o + ents_e\n",
    "        if not all_ents: return\n",
    "        \n",
    "        common = [w for w, c in Counter(all_ents).most_common(20)]\n",
    "        \n",
    "        # Count in each\n",
    "        c_o = Counter(ents_o)\n",
    "        c_e = Counter(ents_e)\n",
    "        \n",
    "        data = []\n",
    "        for ent in common:\n",
    "            data.append([c_o[ent], c_e[ent]])\n",
    "            \n",
    "        if not data: return\n",
    "        \n",
    "        plt.figure(figsize=(8, 10))\n",
    "        sns.heatmap(data, annot=True, fmt=\"d\", cmap=\"YlGnBu\", \n",
    "                    yticklabels=common, xticklabels=[\"Original\", \"Rewrite\"])\n",
    "        plt.title(f\"Entity Overlap Heatmap: {pair_id}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{pair_id}_entity_overlap.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_similarity_matrix(self, sents_o, sents_e, pair_id):\n",
    "        if not sents_o or not sents_e: return\n",
    "        \n",
    "        # Limit to first 20 sentences for readability if too large\n",
    "        so = sents_o[:20]\n",
    "        se = sents_e[:20]\n",
    "        \n",
    "        emb_o = self.sbert.encode(so, convert_to_tensor=True)\n",
    "        emb_e = self.sbert.encode(se, convert_to_tensor=True)\n",
    "        \n",
    "        sim_matrix = util.cos_sim(emb_o, emb_e).cpu().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(sim_matrix, annot=False, cmap=\"YlGnBu\", \n",
    "                    xticklabels=[f\"E{i+1}\" for i in range(len(se))],\n",
    "                    yticklabels=[f\"O{i+1}\" for i in range(len(so))])\n",
    "        plt.title(f\"Sentence Similarity Matrix (Top 20): {pair_id}\")\n",
    "        plt.xlabel(\"Rewrite Sentences\")\n",
    "        plt.ylabel(\"Original Sentences\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{pair_id}_similarity_matrix.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_alignment_table(self, sents_o, sents_e, pair_id):\n",
    "        aligned = self.align_sentences(sents_o, sents_e)\n",
    "        if not aligned: return\n",
    "        \n",
    "        # Sort by score desc and take top 10\n",
    "        aligned.sort(key=lambda x: x[2], reverse=True)\n",
    "        top_10 = aligned[:10]\n",
    "        \n",
    "        # Prepare data for table\n",
    "        cell_text = []\n",
    "        for so, se, score in top_10:\n",
    "            # Truncate for display\n",
    "            so_trunc = (so[:50] + '...') if len(so) > 50 else so\n",
    "            se_trunc = (se[:50] + '...') if len(se) > 50 else se\n",
    "            cell_text.append([so_trunc, se_trunc, f\"{score:.2f}\"])\n",
    "            \n",
    "        if not cell_text: return\n",
    "        \n",
    "        plt.figure(figsize=(12, len(top_10)*0.8 + 1))\n",
    "        ax = plt.gca()\n",
    "        ax.axis('off')\n",
    "        \n",
    "        table = plt.table(cellText=cell_text, \n",
    "                          colLabels=[\"Original\", \"Rewrite\", \"Sim\"], \n",
    "                          loc='center', cellLoc='left', colWidths=[0.4, 0.4, 0.1])\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 1.5)\n",
    "        \n",
    "        plt.title(f\"Top 10 Aligned Sentences: {pair_id}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{pair_id}_alignment_table.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8d3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "article_o = load_text(\"Articles/o-article1.txt\")\n",
    "\n",
    "article_e = load_text(\"Articles/e-article1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9923a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Models... (This may take a moment)\n",
      "Processing o-article1_vs_e-article1...\n",
      "Done. Saved diff_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "comp = ComprehensiveComparator()\n",
    "\n",
    "op = \"Articles/o-article1.txt\"\n",
    "ep = \"Articles/e-article1.txt\"\n",
    "\n",
    "if os.path.exists(op) and os.path.exists(ep):\n",
    "    pair_id = os.path.basename(op).rsplit('.', 1)[0] + \"_vs_\" + os.path.basename(ep).rsplit('.', 1)[0]\n",
    "    print(f\"Processing {pair_id}...\")\n",
    "    \n",
    "    to = load_text(op)\n",
    "    te = load_text(ep)\n",
    "    \n",
    "    stats_res, singles, viz_data = comp.run_pair(to, te, pair_id)\n",
    "    comp.visualize(viz_data, pair_id)\n",
    "    comp.get_gltr_hist(to, te, pair_id)\n",
    "    \n",
    "    all_rows = []\n",
    "    # Flatten to CSV\n",
    "    for metric, s in stats_res.items():\n",
    "        row = {\"pair_id\": pair_id, \"metric\": metric, \n",
    "                \"mean_original\": s['mean_o'], \"mean_rewrite\": s['mean_e'], \n",
    "                \"diff\": s['diff'], \"boot_CI_low\": s['ci_low'], \"boot_CI_high\": s['ci_high'],\n",
    "                \"p_value\": s['p'], \"effect_size\": s['es']}\n",
    "        all_rows.append(row)\n",
    "    \n",
    "    for metric, val in singles.items():\n",
    "            row = {\"pair_id\": pair_id, \"metric\": metric, \n",
    "                \"mean_original\": val, \"mean_rewrite\": val, \n",
    "                \"diff\": 0, \"boot_CI_low\": 0, \"boot_CI_high\": 0,\n",
    "                \"p_value\": 0, \"effect_size\": 0}\n",
    "            all_rows.append(row)\n",
    "\n",
    "    if all_rows:\n",
    "        keys = all_rows[0].keys()\n",
    "        with open(\"diff_summary.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=keys)\n",
    "            w.writeheader()\n",
    "            w.writerows(all_rows)\n",
    "        print(\"Done. Saved diff_summary.csv\")\n",
    "else:\n",
    "    print(\"Error: One or both files not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b685fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORMATED RESULTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from math import pi\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Helper Functions (Add this block to your script)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def compute_bootstrap_ci(data_a, data_b, n_boot=1000, ci=95):\n",
    "    \"\"\"Calculates Bootstrap Confidence Interval for the difference of means.\"\"\"\n",
    "    data_a = np.array(data_a)\n",
    "    data_b = np.array(data_b)\n",
    "    diffs = []\n",
    "    n = len(data_a)\n",
    "    \n",
    "    # Check if data is sufficient\n",
    "    if n < 2: \n",
    "        return (0, 0)\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        # Resample with replacement\n",
    "        sample_a = np.random.choice(data_a, n, replace=True)\n",
    "        sample_b = np.random.choice(data_b, n, replace=True)\n",
    "        diffs.append(np.mean(sample_b) - np.mean(sample_a))\n",
    "    \n",
    "    alpha = (100 - ci) / 2\n",
    "    lower = np.percentile(diffs, alpha)\n",
    "    upper = np.percentile(diffs, 100 - alpha)\n",
    "    return (lower, upper)\n",
    "\n",
    "def compute_cohens_d(x, y):\n",
    "    \"\"\"Calculates Cohen's d effect size.\"\"\"\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    dof = nx + ny - 2\n",
    "    if dof < 1: return 0\n",
    "    return (np.mean(x) - np.mean(y)) / np.sqrt(((nx-1)*np.std(x, ddof=1) ** 2 + (ny-1)*np.std(y, ddof=1) ** 2) / dof)\n",
    "\n",
    "def generate_final_outputs(df_orig, df_rewrite, entity_data, gltr_data, output_dir='.'):\n",
    "    \"\"\"\n",
    "    Generates the requested CSV summary and Charts.\n",
    "    \n",
    "    Args:\n",
    "        df_orig (pd.DataFrame): DataFrame where columns are metrics and rows are samples (Original text).\n",
    "        df_rewrite (pd.DataFrame): DataFrame where columns are metrics and rows are samples (Rewrite text).\n",
    "        entity_data (dict): Dictionary with keys 'original', 'rewrite', 'overlap' containing counts/scores.\n",
    "        gltr_data (dict): Dictionary with keys 'original', 'rewrite' containing lists of GLTR categories (0,1,2,3).\n",
    "        output_dir (str): Directory to save files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Generate diff_summary.csv\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Generating diff_summary.csv...\")\n",
    "    summary_data = []\n",
    "    \n",
    "    # Identify numeric columns common to both\n",
    "    metrics = [c for c in df_orig.columns if pd.api.types.is_numeric_dtype(df_orig[c])]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        orig_vals = df_orig[metric].dropna()\n",
    "        rew_vals = df_rewrite[metric].dropna()\n",
    "        \n",
    "        if len(orig_vals) == 0 or len(rew_vals) == 0:\n",
    "            continue\n",
    "\n",
    "        mean_orig = np.mean(orig_vals)\n",
    "        mean_rew = np.mean(rew_vals)\n",
    "        diff = mean_rew - mean_orig\n",
    "        \n",
    "        # P-value (paired t-test if lengths match, otherwise independent)\n",
    "        if len(orig_vals) == len(rew_vals):\n",
    "            _, p_val = stats.ttest_rel(orig_vals, rew_vals)\n",
    "        else:\n",
    "            _, p_val = stats.ttest_ind(orig_vals, rew_vals, equal_var=False)\n",
    "            \n",
    "        # Bootstrap CI\n",
    "        ci_lower, ci_upper = compute_bootstrap_ci(orig_vals, rew_vals)\n",
    "        ci_str = f\"[{ci_lower:.3f}, {ci_upper:.3f}]\"\n",
    "        \n",
    "        # Effect Size (Cohen's d)\n",
    "        eff_size = compute_cohens_d(rew_vals, orig_vals)\n",
    "        \n",
    "        summary_data.append({\n",
    "            'metric': metric,\n",
    "            'mean_original': mean_orig,\n",
    "            'mean_rewrite': mean_rew,\n",
    "            'diff': diff,\n",
    "            'boot_CI': ci_str,\n",
    "            'p': p_val,\n",
    "            'effect_size': eff_size\n",
    "        })\n",
    "        \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    # Reorder columns as requested\n",
    "    cols = ['metric', 'mean_original', 'mean_rewrite', 'diff', 'boot_CI', 'p', 'effect_size']\n",
    "    df_summary = df_summary[cols]\n",
    "    df_summary.to_csv(os.path.join(output_dir, 'diff_summary.csv'), index=False)\n",
    "    print(\"diff_summary.csv saved.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Generate radar.png\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Generating radar.png...\")\n",
    "    # Normalize data for radar chart to 0-1 scale so metrics with different scales are visible\n",
    "    categories = list(df_summary['metric'])\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Calculate means for plotting\n",
    "    values_orig = df_summary['mean_original'].tolist()\n",
    "    values_rew = df_summary['mean_rewrite'].tolist()\n",
    "    \n",
    "    # Close the loop for radar chart\n",
    "    values_orig += values_orig[:1]\n",
    "    values_rew += values_rew[:1]\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "    ax.plot(angles, values_orig, linewidth=1, linestyle='solid', label='Original')\n",
    "    ax.fill(angles, values_orig, 'b', alpha=0.1)\n",
    "    ax.plot(angles, values_rew, linewidth=1, linestyle='solid', label='Rewrite')\n",
    "    ax.fill(angles, values_rew, 'r', alpha=0.1)\n",
    "    \n",
    "    plt.xticks(angles[:-1], categories)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Metric Comparison (Means)\")\n",
    "    plt.savefig(os.path.join(output_dir, 'radar.png'))\n",
    "    plt.close()\n",
    "    print(\"radar.png saved.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. Generate entity_overlap.png\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Generating entity_overlap.png...\")\n",
    "    # Assuming entity_data is a dictionary like {'Original': 50, 'Rewrite': 45, 'Overlap': 30}\n",
    "    # Or lists of entities. If counts are provided directly:\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    if isinstance(entity_data, dict):\n",
    "        keys = list(entity_data.keys())\n",
    "        vals = list(entity_data.values())\n",
    "        plt.bar(keys, vals, color=['skyblue', 'salmon', 'lightgreen'])\n",
    "        plt.title(\"Entity Analysis\")\n",
    "        plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(output_dir, 'entity_overlap.png'))\n",
    "    plt.close()\n",
    "    print(\"entity_overlap.png saved.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Generate gltr_hist.png\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Generating gltr_hist.png...\")\n",
    "    # gltr_data expected as {'original': [list of ranks/colors], 'rewrite': [list of ranks/colors]}\n",
    "    # 0=Green (Top10), 1=Yellow (Top100), 2=Red (Top1000), 3=Purple (>1000)\n",
    "    \n",
    "    labels = ['Top 10 (Green)', 'Top 100 (Yellow)', 'Top 1k (Red)', '>1k (Purple)']\n",
    "    \n",
    "    # Helper to count frequencies\n",
    "    def get_freqs(data_list):\n",
    "        counts = [0, 0, 0, 0]\n",
    "        for x in data_list:\n",
    "            if 0 <= x < 4: counts[int(x)] += 1\n",
    "        total = sum(counts) if sum(counts) > 0 else 1\n",
    "        return [x/total for x in counts] # Return percentages\n",
    "\n",
    "    orig_freqs = get_freqs(gltr_data.get('original', []))\n",
    "    rew_freqs = get_freqs(gltr_data.get('rewrite', []))\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    rects1 = ax.bar(x - width/2, orig_freqs, width, label='Original')\n",
    "    rects2 = ax.bar(x + width/2, rew_freqs, width, label='Rewrite')\n",
    "    \n",
    "    ax.set_ylabel('Proportion of Tokens')\n",
    "    ax.set_title('GLTR Distribution (Token Probabilities)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'gltr_hist.png'))\n",
    "    plt.close()\n",
    "    print(\"gltr_hist.png saved.\")\n",
    "    print(\"All outputs generated successfully.\")\n",
    "\n",
    "    # EXECUTION\n",
    "    df_o = pd.DataFrame({'Fluency': np.random.rand(10), 'Coherence': np.random.rand(10)})\n",
    "    df_r = pd.DataFrame({'Fluency': np.random.rand(10) + 0.1, 'Coherence': np.random.rand(10) - 0.1})\n",
    "    \n",
    "    ent_data = {'Original Entities': results[\"O\"][\"\"], 'Rewrite Entities': 90, 'Overlap': 60}\n",
    "    \n",
    "    # 0,1,2,3 correspond to Green, Yellow, Red, Purple buckets\n",
    "    gl_data = {\n",
    "        'original': results[\"O\"][\"gltr_ranks\"],\n",
    "        'rewrite':  results[\"E\"][\"gltr_ranks\"]\n",
    "    }\n",
    "\n",
    "    generate_final_outputs(results['O'], results[\"E\"], ent_data, gl_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
